---
title: "[운영체제] 가상 메모리 (1)"
excerpt: "Virtual Memory (1)"
toc: true
toc_sticky: true
toc_label: "주요 목차"
header:
  teaser: /assets/images/operating-system.png

date: 2022-01-28T16:52:03+09:00

categories:
  - OS

tags:
  - Programming
  - 프로그래밍
  - 컴퓨터
  - Computer
  - Computer science
  - Computer engineering
  - 컴퓨터 공학
  - 컴퓨터 과학
  - Operating System
  - 운영체제
  - Virtual Memory
  - 가상 메모리
  - Demand Paging
  - 요구 페이징
  - Page Fault
  - 페이지 폴트
  - 페이지 부재
  - Performance of Demand Paging
  - 요구 페이징의 성능
  - 요구 페이징의 퍼포먼스
  - Page replacement
  - 페이지 교체
  - 페이지 교체 알고리즘
  - Optimal Algorithm
  - 옵티멀 알고리즘
  - FIFO Algorithm
  - 선입선출 알고리즘
  - LRU Algorithm
  - LRU 알고리즘
  - LFU Algorithm
  - LFU 알고리즘  
 
last_modified_at: 2022-01-28T16:52:03+09:00
---

## Virtual Memory (1)

<div class="notice">
    <h4>
        🔊 이화여자대학교 반효경 교수님의 KOCW 2014년 1학기 운영체제 강의를 들으며 정리한 노트입니다.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;캡쳐한 이미지 중 따로 출처 명시를 하지 않은 이미지 또한 반효경 교수님 강의 자료에 있음을 밝힙니다. 
    </h4>
</div>
> 버츄얼 메모리 기법은 전적으로 운영체제가 관여를 하고 있다.

### Demand Paging

> Demand Paging : 요청이 있으면 그 페이지를 메모리에 올리겠다는 얘기다.

- 실제로 필요할 때 page를 메모리에 올리는 것
  - I/O 양의 감소
  - Memory 사용량 감소
  - 빠른 응답 시간
  - 더 많은 사용자 수용
- Valid / Invalid bit의 사용
  - <u>Invalid의 의미</u>
    - 사용되지 않는 주소 영역인 경우
    - 페이지가 물리적 메모리에 없는 경우
  - 처음에는 모든 page entry가 invalid로 초기화
  - address translation 시에 invalid bit이 set되어 있으면<br>→ <span style='color: #b1a5c8'>**<u>"page fault"</u>**</span>

> 이 챕터부터는 앞 부분에 메모리 관리 기법 중에서 페이징 기법을 사용하는걸로 가정을 한다. 그리고, 실제로도 대부분의 시스템은 페이징 기법을 사용을 하고 있다. 이때 이 페이징 기법은 프로그램이 실행될 때 그 프로세스를 구성하는 주소 공간의 페이지를 전부 메모리에 한꺼번에 올리는게 아니라 '디멘드 페이징 기법' 이라는 것을 사용한다. 즉, 그 페이지가 요청이 됐을 때 그걸 메모리에 올려놓는다는 것이다.
>
> 이렇게 함으로써 얻는 효과는 위와 같다. (I/O 양의 감소 등)<BR>프로그램을 구성하는 주소 공간 중에서 실제로 빈번히 사용되는 부분은 지극히 제한적이다. 특히 좋은 소프트웨어 일수록 방어적으로 소프트웨어를 만들기 때문에, 정말 이상한 사용자가 이상한 짓을 하더라도 문제가 생기지 않도록 하는 그런 방어적인 코드가 프로그램에서 대부분을 차지하고 있다는 것이다. 그래서, 그런 코드는 거의 대부분의 경우 사용이 안되는데 그럼에도 불구하고 그런 페이지들을 한꺼번에 메모리에 올려놓는다면 메모리가 낭비될 것이다.
>
> 거기에 비해서 디멘트 페이징을 쓴다면 필요한 것만 메모리에 올리기 때문에 I/O의 양이 상당히 감소하게 된다. 그리고, 이를 통해서 물리적인 메모리를 사용하는 양이 감소하게 되고, 그러면 현대적인 컴퓨터처럼 멀티 프로그래밍 환경 즉, 프로그램 여러개가 동시에 메모리에 올라가는 환경에서는 더 많은 프로그램, 더 많은 사용자가 동시에 메모리에 올라갈 수 있기 때문에 훨씬 효과적이라는 것이다. 그리고, 이를 통해서 더 빠른 응답시간을 기대할 수가 있겠다.
>
> 물론, 빠른 응답 시간은 생각하는 관점에 따라서는 다를 수도 있다. 프로그램이 시작될 때 메모리에 통째로 올려놓으면 그 다음부터는 디스크에 갔다 올 필요가 없으니까 응답 시간이 안 필요한 것 아니냐? 그렇게 생각할 수도 있다. 그렇지만, 여기서 빠른 응답 시간이라는 것은 시스템 전체적으로 생각하자면 어차피 한정된 메모리 공간에 여러 프로그램이 동시에 실행이 되고, 그러면 메모리에 더 빈번하게 사용될 더 의미 있는 정보를 올려놓으려면 디멘드 페이징을 쓰는게 나을 것이다. 그래서, 한정된 메모리 공간을 더 잘 사용하기 때문에 가능하면 디스크에 I/O가 더 줄어들고 메모리에서 직접 서비스하는 비율이 높아지기 때문에 그래서, 응답시간이 더 좋아진다고 생각하면 되겠다.
>
> 우리가 페이징 기법에서 각각의 페이지 테이블 엔트리마다 Valid / Invaild bit이라는게 있다고 했다.
>
> |             **Memory에 없는 Page의 Page Table**              |
> | :----------------------------------------------------------: |
> | <img src="https://user-images.githubusercontent.com/78403443/151470975-4a8b0729-85ff-4474-82e9-cc9bc3921b67.png" alt="image" style="zoom:50%;" /> |
>
> 위 그림을 보면 하나의 프로그램을 구성하는 논리적인 메모리가 있다. 이 프로그램은 지금 여러개의 페이지들로 구성이 되있고, 페이지 테이블을 통해서 페이지들에 대한 주소 변환 정보가 담겨있고, 물리적인 메모리가 주어져 있다. 그리고, 전 챕터에서는 보지 못했던 그림이 하나 더 있다. 맨 오른쪽에 디스크가 있다`Backing Store`...swap area라고 하는 부분이 있다.
>
> 그래서 이 프로그램을 구성하는 이 페이지들(맨 왼쪽) 중에서 당장 필요한 부분은 디멘드 페이징에 의해서 물리적 메모리에 올라가 있을 것이고, 그렇지 않은 부분은 (맨 오른쪽) Backing store 즉, swap 영역에 내려가 있게 된다.
>
> 그래서, Valid / Invalid bit에서 Invalid의 의미는 페이지가 물리적인 메모리에 없는 경우에... 예를 들면, `A, B, C, D, E, F` 중에서 `A, C, F`는 물리적인 메모리에 올라와 있기 때문에 `0, 2, 5`번 페이지는 Valid`v`로 표시돼있고, 실제로 주소 변환 정보가 의미가 있는 값이 들어있는 것이다.<br>반면에, 나머지 페이지들은 지금 물리적인 메모리에 올라와있지 않고, 그림과 같이 backing store에 내려가있기 때문에 Valid / Invalid bit이 Invalid로 표시돼있다.
>
> 그 다음에 또 한 가지는 이 프로그램이 사용하지 않는 주소 영역이 있을 수가 있다. 그 경우에도 페이지 테이블의 엔트리는 주소 공간의 크기 만큼 만들어지게 되고, 거기에는 Invalid로 또 표시가 된다. 즉, 이 프로그램을 구성하는 페이지는 `A`부터 `F`까지 이다. 그리고, `G`하고 `H` ..`6번, 7번` 페이지는 사용이 안되는 페이지 인데, 이 주소 공간에서 이 만큼의(0~7만큼의) 주소 영역을 지원해주기 때문에 페이지 테이블에는 7번까지 엔트리가 만들어지고, 그래서 사용이 안되는 이런 페이지들(`G`, `H`)에 대해서 지금 Valid / Invalid bit이 Invalid`i`로 표시돼있는 것이 `6번, 7번`의 경우이다.
>
> `1번, 3번, 4번`페이지는 이 프로그램에 의해서 사용이 되는 페이지이다. 그렇지만 Invalid`i`로 표시돼있는데, 이것은 이 `1번, 3번, 4번`페이지들이 물리적인 메모리에 올라와있지 않고, 디스크의 스왑 영역에 내려와있기 때문에 Invalid`i`로 표시한 것이다. 그래서, 주소 변환을 통해서 물리적인 프레임 번호를 얻을 수 있는 경우에만 이렇게 Valid`v`라고 표시가 돼있는 것이다.
>
> 그래서, 우리가 디멘트 페이징을 쓰기 때문에 프로그램을 최초로 실행시키면 이 페이지 테이블에는 전부 엔트리가 Invalid`i`로 표시가 되있을 것이다. 그랬다가 그 페이지가 메모리에 올라가면 Invalid로 표시됐던 비트가 Valid`v`로 바뀌면서 해당하는 페이지 프레임 번호가 엔트리에 적히게 되는 것이다.
>
> CPU가 논리 주소를 주고 "메모리 몇 번지를 보겠다.." 그러면 먼저 주소 변환을 하러 올 것이다. 예를 들면, (그림의) 1번 페이지에 대해서 CPU가 접근을 하려고 (주소 변환을 하려고) 봤더니 Invalid`i`이다. 그러면, 이 페이지`1번, B`가 메모리에 없다는 얘기일 것이다. 그런 경우에는 어떻게 해야될까?<BR>일단, 그 페이지를 디스크에서 메모리로 올려야 할 것이다. 이것은 I/O 작업이다. 사용자 프로세스가 직접 못하는 일이다. 그래서, 이렇게 주소 변환을 하려고 봤는데 Invalid`i`로 표시된 그런 페이지에 대해서는 `page fault`라는 현상이 발생한다. 요청한 페이지가 메모리에 없는 경우를 "page fault가 났다" 이렇게 부르는 것이다. 
>
> page fault가 나면, CPU는 자동적으로 운영체제한테 넘어가게 된다. 이걸 페이지 폴트 트랩이 걸린다고 이야기하는데, 일종의 소프트웨어 인터럽트에 해당하는 것이다. 그래서, 그런 경우에는 운영체제가 CPU를 가지고, Fault난 페이지를 메모리에다가 올리는 그런 작업이 필요할 것이다.
>
> 그래서, 페이지 폴트에 대한 처리 루틴이 운영체제에 정의가 되있다.

#### Page Fault

- invalid page를 접근하면 MMU가 trap을 발생시킴 (page fault trap)

  > 그래서 만약에, 엔트리에 invalid로 표시된 페이지를 접근하면 주소 변환을 해주는 하드웨어가 트랩을 발생시키게 된다.

- Kernel mode로 들어가서 page fault handler가 invoke됨

  > 그러면 CPU가 자동적으로 운영체제한테 넘어오게 되는 것이다. 그러면, 운영체제에 페이지 폴트를 처리하는 코드..페이지 폴트 핸들러가 실행이 시작이 되는 것이다. 

- 다음과 같은 순서로 page fault를 처리한다

  1. Invalid reference? (eg. bad address, protection violation) → abort process.

     > 운영체제에서 잘못된 요청이 아닌지? 예를 들면, 주소가 잘못되있거나, 이 프로세스가 사용하지 않는 주소라던지, 또는 접근 권한에 대한 위반을 한 경우에는 강제로 중단을 시키는 절차가 있고

  2. Get an empty page frame. (없으면 뺏어온다: replace)

     > 그렇지 않고, 정상적인 메모리 요청이라고 하면 그 페이지를 디스크에서 메모리로 올려줘야 될 것이다. 근데 쓰다보면 메모리라는게 다 꽉 찼을수도 있다. 만약에 비어있는 페이지 프레임이 없고, 메모리가 꽉 차 있다면 메모리에서 뭔가 페이지 하나를 쫓아내야지만 그 자리에 지금 요청한 페이지를 올려놓을 수 있을 것이다. 
     >
     > 그래서, 빈 페이지를 하나 획득해야되는데, 만약 없으면 하나를 쫓아내야된다는 것이다.

  3. 해당 페이지를 disk에서 memory로 읽어온다

     > 빈 페이지가 획득이 됐으면 디스크에서 그 페이지를 메모리로 올려놓게 된다. 디스크에서 메모리로 올려놓는 작업은 대단히 느린 작업이다.(보통 메모리보다 디스크가 수십만배에서 백만배까지 느림)

     1. disk I/O가 끝나기까지 이 프로세스는 CPU를 preempt 당함 (block)

        > 그래서, 이러한 페이지 폴트에 대해서 디스크 I/O를 하게 되면 이 프로세스가 CPU를 계속 가지고 있어봐야 CPU가 계속 낭비가 된다. 그래서 전에 이미 말했던 파일 입출력을 하는 경우와 마찬가지로 페이지 폴트가 난 경우에도 CPU를 페이지 폴트난 프로세스로부터 뺏어서 이 프로세스의 상태를 블럭으로 만들어주고, 당장 CPU를 사용할 수 있는 ready상태의 프로세스한테 CPU를 넘겨줘야될 것이다.
        >
        > 그런데, 넘겨주기 전에 일단 디스크한테 "그 페이지를 읽어와라" 라고 디스크 컨트롤러한테 부탁을 해야될 것이다.

     2. Disk read가 끝나면 page tables entry 기록, valid/invalid bit = "valid"

        > 그래서, 부탁을 하고 나중에 디스크 I/O가 끝나게 되면, 다시 인터럽트가 걸려서 운영체제가 CPU를 가지고 "아, 그 페이지 폴트 처리가 끝났구나" 그러면 페이지 테이블 엔트리에다가 valid로 표시를 해놓고, 그리고 해당하는 페이지 프레임 번호를 적어놓게 되고,

     3. ready queue에 process를 insert → dispatch later

  4. 이 프로세스가 CPU를 잡고 다시 running

     > 그런 다음에, 나중에 이 페이지 폴트났던 프로세스가 CPU를 다시 잡게 되면, 그때 메모리 주소 변환을 하면 이번에는 페이지 폴트가 나지 않고, 정상적으로 MMU에 의해서 주소 변환이 될 것이다.

  5. 아까 중단되었던 instruction을 재개

     > 그러면, 이제 그 이후 부분을 계속 실행을 할 수가 있는 것이다.

|              **Steps in Handling a Page Fault**              |
| :----------------------------------------------------------: |
| <img src="https://user-images.githubusercontent.com/78403443/151480448-82b45f86-e785-4af9-96d6-8497b26efe93.png" alt="image" style="zoom:50%;" /> |
|                *위 내용을 보여주는 그림이다.*                |

> 그림을 보면, 메모리 레퍼런스`reference`가 있었는데, 주소 변환을 하려고 봤더니 invalid`i`로 표시가 된 것이다. 그러면, 이 페이지가 메모리에 올라와있지 않다는 얘기니까 트랩`② trap`이 걸려서 CPU가 운영체제`operation system`한테 자동으로 넘어간다. 그러면, 운영체제는 Backing store에 있는 그 페이지를 물리적인 메모리로 올려놓는다`③ page is on backing store, ④ bring in missing page`.(만약에 빈 페이지 프레임이 없으면 뭔가를 쫓아내고 거기다 올려놔야되고) 올려놓는 작업이 다 끝났으면 해당하는 프레임 번호를 엔트리에다가 적어놓고, invalid`i`였던 것을 valid`v`로 바꾸는 것이다`⑤ reset page table`. 그런 다음에 나중에 CPU를 다시 얻어서 주소 변환을 하게 되면`⑥ restart instruction` valid`v`로 되있고, 주소 변환을 정상적으로 해서, 해당하는 물리적인 메모리의 페이지 프레임을 접근할 수가 있게 될 것이다.
>
> 이런식으로 페이지 폴트가 처리가 된다.

#### Performance of Demand Paging

> 근데, 페이지 폴트가 났을 때 디스크를 접근하는건 대단히 오래걸리는 작업이기 때문에 "페이지 폴트가 얼마나 나느냐?" 그거에 따라서 메모리 접근하는 시간이 크게 좌우가 되는 것이다.

- Page Fault Rate `0 ≤ p ≤ 1.0`

  > 그래서, 우리가 페이지 폴트의 비율을 0에서 1 사이 값으로 볼 수가 있을 것이다.

  - `if p = 0`, no page faults

    > 만약 페이지 폴트 비율이 0이라면, 절대 페이지 폴트가 안나고, 메모리에서 다 참조가 되는 경우를 의미

  - `if p = 1`, every reference is a fault

    > 페이지 폴트 비율이 1이라면, 매번 메모리 참조할 때마다 페이지 폴트가 난다는 얘기.
    >
    > 실제로 페이지 폴트 비율이 어느정도되느냐? 시스템에서 조사를 해보면 0.0xx 이런식으로 나오게 된다. 즉, 대부분의 경우는 페이지 폴트가 나지 않고, 메모리로부터 직접 주소 변환을 할 수가 있고, 그렇지 않은 특별한 경우에(메모리에 올라와 있지 않은 경우에) 페이지 폴트가 나서 디스크 접근을 필요로 한다는 것이다.

- Effective Access Time

  > 대부분의 경우는 페이지 폴트가 안나지만, 페이지 폴트가 한 번 났다.. 그러면 이거는 엄청난 시간을 겪어야되는 일이다. 
  >
  > 메모리 접근하는 시간을 우리가 페이지 폴트까지 감안해서 한 번 계산을 해보면 아래와 같이 되는 것이다.

  = (1 - *p*) x memory access<br>&nbsp;&nbsp;&nbsp;+ *p* <span style='color: pink'>(OS & HW page fault overhead<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ [swap page out if needed]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ swap page in<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ OS & HW restart overhead)</span> 

> (1 - *p*)는 페이지 폴트가 안나는 비율이 되겠다. 페이지 폴트가 안나는 비율 만큼은 메모리 접근 시간만 걸리게 되겠고,
>
> *p* 페이지 폴트가 나는 비율 만큼은 그 경우에는 엄청난 시간이 걸리는 것이다. 운영체제로 CPU가 넘어가서 하드웨어적으로 페이지 폴트를 처리하는 그런 오버헤드가 있어야 되고`OS & HW page fault overhead`, 또 만약에 메모리에 빈 공간이 없으면 어떤 페이지 하나를 쫓아내야되고`swap page out if needed`, 그리고 그 자리에 디스크에서 읽어온 페이지를 올려놔야되고`swap page in`, 그 다음에 OS가 또 페이지 테이블에 Valid로 표시하고..그런 작업들이 다 끝나고, 나중에 CPU를 얻으면 restart를 하는`OS & HW restart overhead` 그런 과정이기 때문에... 페이지 폴트가 났을 때는 대단히 오버헤드가 크게 되는 것이다.

##### Free frame이 없는 경우

- Page replacement

  > 아까 빈 페이지가 없는 경우에는 뭔가를 쫓아내야된다고 말했었다. 쫓아내는 걸 '페이지 리플레이스먼트' 라고 부른다. 
  >
  > OS가 하는 업무이다. 어떤 페이지를 메모리에서 쫓아내고, 그 자리에 새로운 페이지를 올려놓을 것인가? 그것을 결정

  - 어떤 frame을 빼앗아올지 결정해야 함
  - 곧바로 사용되지 않을 page를 쫓아내는 것이 좋음
  - 동일한 페이지가 여러 번 메모리에서 쫓겨났다가 다시 들어올 수 있음

- Replacement Algorithm

  > '페이지 리플레이스먼트'를 해주는 알고리즘을 '리플레이스먼트 알고리즘' 이라고 부른다.
  >
  > 이 알고리즘은 가능하면 페이지 폴트가 나지 않고, 메모리에서 직접 처리할 수 있게 해주면 좋을 것이다. 뭔가는 쫓겨나야되는데, 만약에 페이지를 쫓아냈더니 그 페이지가 다시 참조가 된다 그러면, 엄청난 시간을 또 겪어야되기 때문에...

  - page-fault rate을 최소화하는 것이 목표

    > 그래서, Replacement Algorithm은 가급적 Page Fault Rate이 낮아지도록 해야된다는 것이다. 가급적 Page Fault Rate이 0에 가깝도록 해줘야되는게 이 알고리즘의 목표가 되는 것이다.

  - 알고리즘의 평가

    - 주어진 page reference string에 대해 page fault를 얼마나 내는지 조사

  - reference string의 예<br>`1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5`.

    > reference string이라는 것이 있고, 이것은 시간 순서에 따라서 페이지들을 서로 다른 번호를 붙여놓고 페이지들이 참조된 순서를 이렇게 나열해 놓은 것이다.
    >
    > 즉, 처음에 1번 페이지가 참조되고, 그 다음에 2번 페이지, 3번, 4번 페이지.. 그 다음에 다시 1번 페이지, 2번 페이지, 5번 페이지 이런식으로 참조가 됐다는 얘기다.
    >
    > 그래서 만약에, 1번 페이지가 (맨 1번째에서)이미 한번 사용이 됐는데, 이게 메모리에 올라가 있다가 Replacement Algorithm이 이걸 쫓아냈으면 여기서(5번째에서) 페이지 폴트가 또 날거고, 그렇지 않고 맨 첫번째에서 1번 페이지를 쫓아내지 않았으면, 나중에는 메모리에서 직접 참조가 될 수가 있을 것이다.

|                     **Page Replacement**                     |
| :----------------------------------------------------------: |
| <img src="https://user-images.githubusercontent.com/78403443/151486403-7516b942-f005-4d19-a3be-16bedb089975.png" alt="image" style="zoom:50%;" /> |

> 그래서, 페이지 리플레이스먼트라는거는 어떤거를 쫓아낼지를 결정하고, 희생양`victim`이 결정이 됐으면 그 친구를 디스크로 쫓아내는 것이다. 근데, 만약에 이 프로그램이 디스크에서 메모리로 올라온 이후에 내용이 변경됐다고 하면 즉, write가 발생했다고 하면 이 친구를 쫓아내기만 하면 되는게 아니라 쫓아낼 때 변경된 내용을 메모리에서 백킹 스토어에다 써줘야된다.<br>근데, 아까 이 백킹 스토어에 있던 걸 메모리에 올린 다음에 이 친구가 쫓겨날 때까지 변경된게 없다 그러면, 물리적 메모리에서 지워버리기만 하면 되는 것이다.
>
> 그래서, 어떤 `victim`이 선정되면 일단 쫓아내고`① swap out victim page` 그 자리에 새로운 페이지를 올려놔야 될 것이다`③ swap desired page in`. 그럼 이 단계에서 쫓겨난 페이지에 대한 테이블 엔트리에는 비트를 invalid`i`로 바꿔줘야겠고`② change to invalid`, 그 다음에 메모리에 올라온 페이지에 대해서는 그 프레임 번호`f`를 엔트리에 적어주고 그리고, 이 비트를 valid`v`로 바꿔주면 되는 것이다`④ reset page table for new page`.
>
> 이 역할을 운영체제가 하게 되는 것이다.

###### Optimal Algorithm

> 그러면 도대체 어떤 알고리즘이 가장 좋은 알고리즘이냐?<BR>알고리즘 중에서 가장 좋은 알고리즘이 바로 이 '옵티멀 알고리즘'이라는 것이다.
>
> 이것은 페이지 폴트를 가장 적게하는 알고리즘이다.
>
> 근데, 실제 시스템에서는 사실 미래를 알 수가 없다. 예를 들면, 1번 페이지가 참조가 됐다 그러면 미래에 1번 페이지가 언제 참조될지를 모르기 때문에, 이것을 쫓아내야될지 말아야될지를 결정을 하기가 어려운데,

- MIN (OPT): 가장 먼 미래에 참조되는 page를 replace

  > 이 옵티멀 알고리즘은 미래에 참조되는 페이지들을 미리 다 안다고 가정을 하는 것이다. 그래서, 실제 시스템에서 사용될 수는 없다.

- 4 frames example<br>`1,  2,  3,  4,  1,  2,  5,  1,  2,  3,  4,  5`

  <img src="https://user-images.githubusercontent.com/78403443/151492235-65d6d257-183e-46cc-8f88-a585718bb0d8.png" alt="image" style="zoom:50%;" />

- 미래의 참조를 어떻게 아는가?

  - Offline algorithm

    > 그래서 이 알고리즘을 우리가 특별히 '오프라인 옵티멀 알고리즘' 이라고 부른다. 
    >
    > 이 오프라인이라는 것은 실제 시스템에서 온라인으로 사용하는게 아니라 페이지 레퍼런스 스트링(`1,  2,  3,  4,  1,  2,  5,  1,  2,  3,  4,  5`)을 미리 알고 있다는 가정 하에 이 알고리즘을 운영을 하는 것이다.

    > 그러면, 이 알고리즘이 어떻게 운영되는가? 설명을 하면, 위 그림과 같이 운영을 한다. 가장 먼 미래에 참조되는 페이지를 쫓아낸다. 그러면 한번 그 상황이 생겼을 때 어떻게 동작하는지 한번 보자.
    >
    > 처음에는 메모리가 다 비어있을 것이다. 그러니까 <span style='color: pink'>**1**</span>번 페이지는 아무리 옵티멀 알고리즘이라도 페이지 폴트가 난다. 그래서 `1`번이 메모리에 올라오고, 그 다음 <span style='color: pink'>**2**</span>번도 처음 참조됐으니까 페이지 폴트가 나서 메모리에 올라오고 <span style='color: pink'>**3**</span>번, <span style='color: pink'>**4**</span>번도 마찬가지다.<br>그 다음에 **1**번이 다시 참조가 된다. 그러면 이것은 이미 메모리에 올라와있기 때문에 페이지 폴트가 나지 않고, 메모리에서 직접 참조가 된다. (위 그림에 <span style='background-color: red'>빨간색</span>은 페이지 폴트가 난 경우를 표시한 것이고, <span style='background-color: #a840ff'>연보라색</span>은 이 참조에 대해서 페이지 폴트가 나지 않고, 메모리에서 직접 참조된 경우를 나타내고 있다)<br>**2**번도 마찬가지다 이미 메모리에 올라와있기 때문에, 직접 참조가 된다.<br>그 다음에 <span style='color: pink'>**5**</span>번이 요청이 들어왔는데, 이 <span style='color: pink'>**5**</span>번은 지금 메모리에 없다, `1, 2, 3, 4`만 있기 때문에... 그래서 페이지 폴트가 나고, 그러면 이제 메모리가 꽉 찼기 때문에 `1, 2, 3, 4`중에 하나를 쫓아내야지 <span style='color: pink'>**5**</span>번이 메모리에 올라갈 수가 있다.  
    >
    > 그때, 이 오프라인 옵티멀 알고리즘은 `1, 2, 3, 4`중에 어떤걸 쫓아내느냐 하면 가장 먼 미래에 참조되는 페이지를 쫓아낸다. 미래를 볼 수 있기 때문에 미래를 보는 것이다. `1`번은 바로 다음에 참조가 된다. 그러니까 안쫓겨나고, `2`번이 그 다음에 또 참조되니까 안쫓겨남, `3`번이 그 다음에 참조되니까 `3`번도 살았다. `4`번이 그 다음에 나오긴 하지만, 네 개 중에서는 가장 먼 미래에 참조되는게 `4`번이라는 것이다. 그래서 <span style='color: pink'>**5**</span>번을 메모리에 보관하기 위해서 `1, 2, 3, 4`중에서 **4**번을 쫓아내고, 그 자리에 <span style='color: pink'>**5**</span>번을 집어넣게 된다. 
    >
    > 이런식으로 운영을 하는게 '오프라인 옵티멀 알고리즘' 이고, 이 알고리즘을 사용하는 경우에 쭉 따라가보면 총 6번의 페이지 폴트`6 page faults`를 발생시키게 된다.
    >
    > 이게 '옵티멀 알고리즘'이라고 했기 때문에, 어떤 알고리즘을 쓰더라도 페이지 폴트를 6번보다 더 적게 낼 수는 없다는 의미이다. 수식을 통해서 증명을 할 수도 있지만 상식적으로 생각해보면, 가장 먼 미래에 참조되는 페이지를 쫓아내면 그게 "페이지 폴트를 가장 적게할 것이다" 라는 것을 짐작할 수 있다.

- 다른 알고리즘의 성능에 대한 upper bound 제공

  > 어쨋든, 이 알고리즘은 미래를 다 안다고 가정하기 때문에, 실제 시스템에서 사용하는 것은 불가능하고 다만, 실제 시스템에서 사용하는 다른 알고리즘들에 대한 성능의 upper bound를 제공한다는 것이다. 
  >
  > 이게 무슨 이야기냐... 아무리 좋은 알고리즘을 만들어도 이거보다 성능이 더 좋을 수가 없다는 얘기다. 내가 알고리즘을 하나 만들었다, 근데 그 성능이 오프라인 옵티멀 알고리즘하고 거의 비슷한 성능이 나왔다. 그러면 더이상 좋은 알고리즘을 만들 수가 없다는 이야기다.
  >
  > upper bound를 제공하기 때문에 참고로 사용하는 알고리즘이라는 것이고,

  - Belady's optimal algorithm, MIN, OPT 등으로 불림

    > 이 오프라인 옵티멀 알고리즘을 Belady(빌레디)라는 사람이 만들어서 Belady's optimal algorithm 이렇게도 부르고 또는, 페이지 폴트를 제일 적게 낸다고해서 MIN 알고리즘이라고도 부르고, 또 옵티멀이니까 줄여서 OPT(옵트) 이런식으로 이 알고리즘 이름을 부르기도 한다.

###### FIFO (First In First Out) Algorithm

> 여기부터는 실제 시스템에서 사용 가능한 즉, 미래를 모르는 상태에서 운영하는 알고리즘들에 대해서 나온다.
>
> 우리가 미래를 모를 때, 미래를 모르지만 미래를 잘 예측해야된다. 왜냐하면, 지금 상황이 어떤 페이지를 쫓아내면 미래에 참조가 안 될 페이지를 잘 쫓아낸 것인지를 예측해야되는 것이기 때문이다. 그러면, 미래를 모를 때 제일 참고할만한 중요한 단서가 뭘까? 미래를 모를 때는 과거를 보면 된다.
>
> 일단 FIFO 알고리즘을 먼저 설명한다. 이 알고리즘은 First In First Out(선입선출) 방식으로 운영되는 알고리즘이다.

- FIFO: 먼저 들어온 것을 먼저 내쫓음

  > 즉, 메모리에 먼저 들어온 페이지를 먼저 쫓아내는 방식이라는 것이다.

<img src="https://user-images.githubusercontent.com/78403443/151498833-195b0c42-31d5-4ea5-a68b-577053087512.png" alt="image" style="zoom:50%;" />

> 그림은 앞에서 봤던 동일한 레퍼런스 스트링에 대해서 메모리에 페이지 프레임이 3개 있는 경우하고, 4개 있는 경우에 FIFO 알고리즘으로 페이지 운영을 쭉 해본 것이다. 그래서 먼저 들어온 거를 만약에 쫓아내야되는 상황에서는 쫓아내는 방법이다.
>
> 이 알고리즘은 아주 특이한 성질이 하나 있다.<BR>그게 뭐냐하면 메모리를 3페이지 프레임에서 4페이지 프레임으로 메모리 크기를 늘려주면 보통은 성능이 좋아져야될텐데, 그러나 메모리 크기를 늘려주는데 성능이 더 나빠지는 즉, 페이지 폴트 수가 더 늘어나는 이런 상황이 발생할 수가 있다는 것이다.

- FIFO Anomaly (Belady's Anomaly)

  > 이러한 기이한 현상을 FIFO Anomaly다. "FIFO 알고리즘의 특이한 현상이다"해서 이렇게 부르고, 또 Belady(빌레디)라는 사람이 얘기한 것이기 때문에 Belady's Anomaly다 이렇게도 부른다.

  - more frames ≠> less page faults

    > 이 현상은 메모리 프레임을 늘려줬는데 성능이 더 나빠지는 그런 경우가 발생할 수가 있기 때문에 이런 나쁜 성질을 이야기하는 것이다.

###### LRU (Least Recently Used) Algorithm

> 실제로는 FIFO 알고리즘 같이 Anomaly 같은 현상이 발생 안하고, 가장 메모리 관리나 이런데서 제일 많이 쓰는 알고리즘이 여기에 소개되는 LRU(Least Recently Used) 알고리즘이다.
>
> 가장 최근에 덜 사용된 그런 페이지를 쫓아내는 알고리즘이다.<BR>제일 오래 전에 사용된걸 쫓아내겠다는 얘기이다.

- LRU: 가장 오래 전에 참조된 것을 지움

  > 먼저 들어왔지만 들어온 다음에 재사용이 되면, 최근에 사용된거니까 안쫓아낸다는 것이다. 

<img src="https://user-images.githubusercontent.com/78403443/151500991-6d416fee-8ac8-4371-871f-fcc80863a5b3.png" alt="image" style="zoom:50%;" />

> 운영을 해보자. 그림을 보면 처음에 <span style='color: pink'>**1, 2, 3, 4**</span>가 들어올 때는 어쩔 수 없이 처음이니까 페이지 폴트가 나고, 그 다음에 **1, 2**는 이미 메모리에 올라와있으니까 메모리에서 직접 참조가 된다. 그 다음에 <span style='color: pink'>**5**</span>번을 메모리에 보관하기 위해서 `1, 2, 3, 4`중에 뭔가 하나를 쫓아내야한다. (근데, 지금 이 알고리즘들은 온라인 알고리즘이기 때문에 미래는 모른다. 실제 시스템에서 사용하려면 미래는 모른다. 과거를 가지고 어떤걸 쫓아낼지 결정할 수가 있는데, 이 LRU 알고리즘은 가장 오래 전에 사용된 페이지를 쫓아낸다는 것이다.)
>
> 보면 `1, 2, 3, 4`중에 `2`번이 가장 최근에 사용됐다. 그렇기 때문에 `2`번은 살아남았고 그거보다 조금 전에 `1`번이 사용됐으니까 `1`번도 살아남았고, 그 전에 `4`번이 사용됐으니까 `4`번도 살아남았고, 가장 오래 전에 사용된게 `3`번이다. 
>
> 그래서, 이 LRU 알고리즘은 하나를 쫓아내야될 때 가장 오래 전에 사용된 `3`번을 쫓아내고, 그 자리에  <span style='color: pink'>**5**</span>번을 집어넣는 것이다.
>
> FIFO 알고리즘인 경우에는 조금 다르다.
>
> <img src="https://user-images.githubusercontent.com/78403443/151498833-195b0c42-31d5-4ea5-a68b-577053087512.png" alt="image" style="zoom:50%;" />
>
> 네 개의 페이지 프레임일 때 `1, 2, 3, 4, 1, 2, 5` <span style='color: pink'>**5**</span>번이 참조 될 때, 가장 먼저 메모리에 들어온 것은 `1`번이기 때문에 `1`번을 쫓아내고, 그 자리에 <span style='color: pink'>**5**</SPAN>번을 집어 넣었다. 그러니까 `1`번이 메모리에 들어온 이후에 들어올 때만 사용된게 아니고, 이후에(다섯번째에서) 또 사용이 되었는데도 불구하고, 단지 먼저 들어왔다는 이유로 먼저 쫓겨난게 FIFO 알고리즘이고...
>
> LRU 알고리즘은
>
> <img src="https://user-images.githubusercontent.com/78403443/151500991-6d416fee-8ac8-4371-871f-fcc80863a5b3.png" alt="image" style="zoom:50%;" />
>
> 들어온 기준이 아니고 실제로 (들어온 것 포함해서) 들어온 이후에 사용된 것 기준으로 가장 오래전에 사용된 그런 페이지를 쫓아낸다는 것이다.
>
> 최근에 참조된 페이지가 가까운 미래에 참조될 가능성이 높은 성질을 이용하는 것이다. 프로그램이 참조하는 페이지들도 최근에 참조된 페이지가 다시 참조되는 성향이 높더라는 것이다. 그래서, 이런 LRU 알고리즘을 쓰면 비교적 page fault rate을 줄일 수가 있다는 얘기다.

###### LFU (Least Frequently Used) Algorithm

> LFU 알고리즘은 가장 덜 빈번하게 사용된 페이지를 쫓아내자는 것이다.

- LFU: 참조 횟수(reference count)가 가장 적은 페이지를 지움

  > 참조 빈도.. 다른 말로 참조 횟수라고 부를 수가 있고, LFU 알고리즘은 참조 횟수가 제일 적은 페이지를 메모리에서 쫓아낸다.
  >
  > 일상생활에서도 참고 할만한 성질이지만, 실제로 프로그램에서도 이런 성질이 나타나기 때문에 이런걸 활용하는 것이다.
  >
  > 그래서, 과거에 참조 횟수가 많았던 페이지는 미래에 다시 참조될 가능성이 높다. 그런 페이지는 쫓아내지 말자는 것이다.

  - 최저 참조 횟수인 page가 여럿 있는 경우

    > 참조 횟수가 가장 적은 페이지가 동률로 여러개 있을 수도 있다.<BR>그런 경우에 LFU 알고리즘은 어떤걸 쫓아낸다고 특별히 명시하고 있지는 않다. 

    - LFU 알고리즘 자체에서는 여러 page 중 임의로 선정한다

      > 아무거나 쫓아내도 참조 횟수가 제일 적은걸 쫓아낸거니까 상관은 없는데,

    - 성능 향상을 위해 가장 오래 전에 참조된 page를 지우게 구현할 수도 있다

    > 우리가 조금 더 성능을 높이고 싶다면 참조 횟수가 동률인 페이지 중에서 마지막 참조 시점이 더 오래된 것을 쫓아내는게 조금 더 나을 것이다.

  - 장단점

    - LRU처럼 직전 참조 시점만 보는 것이 아니라 장기적인 시간 규모를 보기 때문에 page의 인기도를 좀 더 정확히 반영할 수 있음
    - 참조 시점의 최근성을 반영하지 못함
    - LRU보다 구현이 복잡함

###### LRU와 LFU 알고리즘 예제

<img src="https://user-images.githubusercontent.com/78403443/151504652-00f1d921-54a9-411a-8f8d-89dd223cec45.png" alt="image" style="zoom:50%;" />

> LRU 알고리즘과 LFU 알고리즘 이 2가지 알고리즘은 장단점이 있다. 이 예제를 통해서 설명한다.
>
> 그림을 보면 페이지를 위한 레퍼런스 스트링이 쭉 주어져 있고<BR>`1, 1, 1, 1, 2, 2, 3, 3, 2, 4, 5, ...`,<BR>그 다음에 페이지 프레임`Page Frames`은 4개가 있다. <BR>그리고 우측의 그림은 이 페이지들이`page 1, page 2, page 3, page 4` 시간에 따라서 참조된 이런(`1, 1, 1, 1, 2, 2, 3, 3, 2, 4, 5, ...`) 순서대로 각 페이지 별로 화살표를 표시해 놓은 것이다. 즉, 1번 페이지`page 1`가 먼저 4차례 참조가 됐고, 그 다음에 2번 페이지`page 2`가 2번 참조가 되고, 3번 페이지`page 3`가 또 2번 참조가 된 다음에, 다시 2번 페이지`page 2`가 1번, 4번 페이지`page 4` 1번 참조되고.. 현재시각이 여기(굵은 파란선)라는 것이다.
>
> 이(현재시각) 시점에 `1, 2, 3, 4`가 모두 메모리에 올라와있는 상태인데, 현재 **5**번 페이지가 참조가 되기 때문에 이 4개의 페이지`page 1, page 2, page 3, page 4` 중에 어느 하나를 쫓아내야되는 상황이다.
>
> 그럼, LRU 알고리즘은 이 중에 1번 페이지를 쫓아낸다. 왜냐하면, 현재 시점(현재시각 시점)부터 마지막 참조 시점까지의 거리가 `page 4`가 제일 가까우니까 `page 4`가 제일 우선순위가 높고, 그 다음이 `page 2` 그 다음이 `page 3` 그 다음 `page 1`이 제일 오래되었기 때문에 1번 페이지`page 1`를 쫓아내는 것이다.<BR>근데, 이 LRU 알고리즘은 마지막 참조 시점만 보기 때문에, 그 이전에 어떤 기록을 가지고 있는지를 전혀 생각하지 않는다는게 약점이다. 즉, `page 1`은 가장 오래 전에 참조되긴 했지만, 굉장히 많은 참조가 과거에 있었다는 사실을 LRU는 전혀 고려하지 않는다. 그게 문제점이다.
>
> 반대로, LFU 알고리즘은 참조 횟수를 기준으로 `page 1, page 2, page 3, page 4`순으로 4번, 3번, 2번, 1번이니까 제일 참조 횟수가 적은 4번 페이지` page 4`를 쫓아낼 것이다. 근데, 이 알고리즘은 또 문제점이 비록 4번`page 4`이 참조횟수가 1번이지만, 이제 막 여러번의 참조가 시작되는 그런 상황인데 이 친구`page 4`를 쫓아내면 이것도 또 문제가 될 것이다. 이게 LFU 알고리즘의 단점이다.
>
> 서로 간에 장단점이 있다. 그래서, 이 두 알고리즘의 장단점을 보완하는 Page replacement알고리즘에 대해서는 지난 한 20년이 넘게 대단히 많은 연구가 논문으로도 나오고 또 실제로도 시스템에서 개발이 된 바가 있다.

###### LRU와 LFU 알고리즘의 구현

그럼 이 알고리즘들이 어떻게 구현이 되는가? 구현 방식에 대해서 설명을 할 것이다.

<img src="https://user-images.githubusercontent.com/78403443/151729677-8106c5ee-15d9-4849-a4d8-328f1c928e6a.png" alt="image" style="zoom:50%;" />

**LRU 알고리즘**은 이와 같이 메모리 안에 있는 페이지들을 참조 시간 순서에 따라서 한 줄로 줄 세우기를 한다.<BR>그래서, 맨 위에 있는 페이지가 가장 오래전에 참조된 페이지이고, 아래로 갈수록 참조 시점이 더 최근이고, 제일 아래쪽에 페이지는 가장 최근에 참조된 페이지다. 이게 링크드 리스트(Linked List)형태로 운영체제가 페이지들의 참조 시간 순서를 관리하는 것이다.

그래서, 만약에 어떤 페이지가 메모리에 들어오거나 또는 이 메모리 안에서 다시 참조가 되거나 하면 그 페이지는 참조 시점이 가장 최근이다.

<img src="https://user-images.githubusercontent.com/78403443/151729920-aa02a321-353c-4eff-aa1e-112030092301.png" alt="image" style="zoom:50%;" />

그러니까 그 페이지는 볼 것도 없이 제일 아래쪽으로 보내면 된다.(이게 Doubly Linked List(이중 연결 리스트)형태로 만들면 최근에 참조된 위에서 두번째꺼를 맨 아래로 빼가지고, 위에서 첫번째꺼와 세번째꺼를 연결하는건 별로 어려운 일이 아니다. 그래서 이 친구`New reference`는 제일 아래 쪽으로 이동을 시킴)<br>그 다음에 replacement(리플레이스먼트)... 쫓아내야될 때는 제일 위에 있는걸 쫓아내면 되겠다.

그래서, 이 LRU 알고리즘은 이런식으로 리스트 형태로 구현을 하면 시간 복잡도가 O(1)`order of 1`이 된다.

<img src="https://user-images.githubusercontent.com/78403443/151730430-81def0d8-7e7a-48a5-a6f7-a5fed0d7cb6e.png" alt="image" style="zoom:50%;" />

O(1) 이라는건 쫓아내기 위해서 비교가 필요 없다는 얘기다. 

우리가 이 LRU 알고리즘을 이런식으로 구현하지 않고, 만약에 어떤 페이지가 참조될 때마다 그 시간을 기록해놓은 다음에 어떤 페이지를 쫓아낼까 결정할 때는 시간을 다 비교해보고, 그 중에 제일 타임 스탬프가 오래된 페이지를 쫓아내는 방식으로 구현한다고 하면 매번 어떤거를 쫓아낼지를 결정하기 위해서 만약에 이 메모리의 페이지 개수가 n개 라고 하면, O(n) 의 시간이 걸린다. n개의 시간을 다 비교해보고, 가장 오래된거를 쫓아내야된다는 것이다. 

근데, 이렇게 하면 대단히 비효율적이다. 이 리플레이스먼트 알고리즘이라는 것은 그때그때 어떤걸 쫓아낼지를 바로 결정해야되는데, 페이지 수가 n개...예를 들어서, 100만개가 된다면, 100만개 페이지의 참조 시간 순서를 비교해보는거는 상당히 오래 걸리는 작업이니까... 그래서 그런식으로 하는 것보다 어떤 페이지가 참조될 때마다 그 친구를 제일 아래쪽에다가 매달고 쫓아낼 때는 맨 위에 있는걸 쫓아내고 이러면 비교가 전혀 필요 없을 것이다.

그래서, LRU 알고리즘은 이런식으로 링크드 리스트 형태로 O(1)의 시간에 구현을 하는 방법을 사용하게 된다.

그러면 **LFU 알고리즘**은 어떻게 구현을 할 수가 있을까?

<img src="https://user-images.githubusercontent.com/78403443/151730999-3403ad16-11ae-46de-a530-ffe795dd2979.png" alt="image" style="zoom:50%;" />

비슷하게 한 줄로 줄 세우기를 해서 구현을 생각해볼 수가 있겠다.<BR>즉, 가장 참조 횟수가 적은 페이지가 맨 위에 있고, 밑으로 내려갈수록 참조 횟수가 더 많은 페이지를 이렇게 위치를 해놓고, 쫓아낼 때는 가장 참조횟수가 적은 페이지를 쫓아내겠다. 뭐 가능할 것 같은데... 사실은 이런 한 줄로 줄세우기를 할 수가 없다.

<img src="https://user-images.githubusercontent.com/78403443/151731181-ffc00acd-6c0e-4f69-bf09-77c1bb5812e4.png" alt="image" style="zoom:50%;" />

왜냐하면, 어떤 페이지가 참조가 됐을 때 LRU 알고리즘은 1번만 참조되도 지금 참조된 페이지가 제일 가치가 높다. 가장 최근에 참조된거니까...시간 순서로 따지는거기 때문에, 1번만 참조되면 그 친구가 왕이다. 제일 아래로 내려올 수가 있는데...

LFU 알고리즘은 이 친구`New reference`가 참조됐다는건 참조 횟수가 1이 늘어난 것이다. 그래서, 참조 횟수가 1이 늘어났다고해서 가장 참조 횟수가 많은 페이지가 아니다. 그래서 이 친구`New reference`는 제일 밑으로 내려올 수 있는게 아니라

<img src="https://user-images.githubusercontent.com/78403443/151731572-6ad56255-6d73-4cbb-960c-7f1568113b2f.png" alt="image" style="zoom:50%;" />

비교를 해서 어디까지 내려갈 수 있는지를 확인해봐야 된다. 

최악의 경우에는 여기 있는 페이지들이 전부 참조 횟수가 같았다고 하면, 1번의 참조로 인해서 제일 아래쪽까지 내려가야될 수도 있을 것이다. 그렇지만, 그냥 내려갈 수 있는게 아니라 하나하나 비교를 해서 내가 과연 어느 친구까지 이겨낼 수 있는가? 내가 너를 이길 수 있다. 그러면 자리 바꿈을 하고, 또 다음 친구하고 횟수적으로 싸워서 이길 수 있다면 자리 바꿈을 하고, 또 싸워서 내가 더 참조 횟수가 많네? 자리 바꿈을 하고...다시 맨 아래에 있는 친구하고 비교해보니까 이제는, 맨 아래 페이지가 참조 횟수가 더 많다 그러면 나`New reference`는 맨 아래로부터 두번째 자리까지 내려갈 것이다.

그래서, 이런식으로 구현을 하면

<img src="https://user-images.githubusercontent.com/78403443/151732132-f0a7a5df-58a2-47d4-9c4b-f846f002f79f.png" alt="image" style="zoom:50%;" />

LFU 알고리즘은 이 안에 페이지의 갯수가 n개라고 하면, O(n)`order of n`의 시간이 걸린다. 

그래서, LFU 알고리즘은 이런식으로 구현을 하지 않고...

<img src="https://user-images.githubusercontent.com/78403443/151732304-122146fc-6199-42e4-bf9c-7b1a980b6c2f.png" alt="image" style="zoom:50%;" />

heap(힙)이라는 자료구조를 이용해서 구현을 하게 되고, 그러면 O(log n)`order of log n`의 구현이 가능하다. 

<img src="https://user-images.githubusercontent.com/78403443/151732429-6c90b1c4-f6a0-4082-ad3d-17413f20f263.png" alt="image" style="zoom:50%;" />

힙은 이런식으로 생겼다. 자식이 2개씩 있는 이러한 이진트리 형태로 되있다.(complete binary tree 이다)

그래서, 맨 위에는 참조 횟수가 제일 적은 페이지를 놓고, 부모보다는 자식이 참조 횟수가 더 많은 페이지...즉, 밑으로 갈수록 참조 횟수가 더 많은 페이지를 놓는 것이다.

<img src="https://user-images.githubusercontent.com/78403443/151732719-f99f8002-6fe4-4993-982d-50c68850e7c5.png" alt="image" style="zoom:50%;" />

그럼 이 상황에서 어떤 페이지가 참조가 됐다`New reference`. 그러면 참조 횟수가 1이 늘어났으니까 밑으로 내려갈 수가 있을 것이다. 근데, 한 줄로 줄세우기를 했을 때하고 다르게 이렇게 세워놓으면, 이 친구`New reference`하고 직계자식 2개 하고만 비교를 한다.<br>그래서, 둘 중에 내가 지금 1번 참조가 늘어났기 때문에, 참조 횟수가 더 많아졌다 그러면...

<img src="https://user-images.githubusercontent.com/78403443/151733095-1fc75a89-059e-41d3-89bc-c1d571c9cb2b.png" alt="image" style="zoom:50%;" />

자리 바꿈을 하는 것이다.(아래에 있는게 위로 올라가고, 내가`New reference` 참조 횟수가 1 늘어났으니까 내려가고..) 그 다음에 또 더 내려갈 수 있는지 자식 둘 하고 비교를 해봐서 참조 횟수가 아직도 더 많다 그러면, 자리 바꿈을 해서 내려가는 것이다. 

<img src="https://user-images.githubusercontent.com/78403443/151734537-a87f8463-b16a-4421-bdf9-a7bdfcc928c8.png" alt="image" style="zoom:50%;" />

그래서, 언제까지 내려갈 수 있느냐? 이제는 자식 둘 보다 내가 참조 횟수가 더 많지가 않으면 여기(그림 화살표 마지막)까지만 내려갈 수가 있는 것이다. 그래서, 해당하는 경로만 따라 내려가면서 내가 어디까지 내려갈 수 있는지를 찾아보는 것이다.

<img src="https://user-images.githubusercontent.com/78403443/151734742-cc57f70c-278f-4fcf-8847-c0ee88888a51.png" alt="image" style="zoom:50%;" />

근데, 이 전체가 n개라고 하면 이런 이진트리를 구성하면 높이가 log<sub>2</sub>n이 된다. 그래서, 비교 횟수가 많아봐야 `log n`이 되는 것이다. 그리고, 쫓아낼 때는 root에(맨 위에) 있는 것을 쫓아내고, 힙을 재구성하면 된다.

> `n`하고 `log n`은 굉장히 다르다. 
>
> <img src="https://user-images.githubusercontent.com/78403443/151734939-6742e588-169d-4a5a-92d3-dc6b216d9b19.png" alt="image" style="zoom:50%;" />
>
> n이 100만이라고 하면 이렇게 하면 100만번의 비교가 필요할 수가 있는데,
>
> <img src="https://user-images.githubusercontent.com/78403443/151734742-cc57f70c-278f-4fcf-8847-c0ee88888a51.png" alt="image" style="zoom:50%;" />
>
> log<sub>2</sub>에 100만을 하면 이 값이 10에서 20사이의 숫자가 나온다. 그래서, 비교 횟수가 열 몇번으로 크게 줄어든다. 즉, 100만번 비교할게 열 몇번으로 줄어든다. 
>
> 그래서, 이 알고리즘은 항상 적어도 O(log n)`order of log n`이하는 되어야지만 리플레이스먼트 알고리즘(Replacement Algorithm)으로 쓸 수 있지, O(n)`order of n`인 그런식으로는 구현이 안된다는 것이다.

##### 다양한 캐싱 환경

- 캐싱 기법
  - 한정된 빠른 공간(=캐시)에 요청된 데이터를 저장해 두었다가 후속 요청시 캐시로부터 직접 서비스하는 방식
  - Paging system 외에도 cache memory, buffer caching, Web caching 등 다양한 분야에서 사용
- 캐시 운영의 시간 제약
  - 교체 알고리즘에서 삭제할 항목을 결정하는 일에 지나치게 많은 시간이 걸리는 경우 실제 시스템에서 사용할 수 없음
  - Buffer caching이나 Web caching의 경우
    - O(1)에서 O(log n) 정도까지 허용
  - Paging system인 경우
    - page fault인 경우에만 OS가 관여함
    - 페이지가 이미 메모리에 존재하는 경우 참조시각 등의 정보를 OS가 알 수 없음
    - O(1)인 LRU의 list 조작조차 불가능

**[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}**